import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 1ï¸âƒ£ åŠ è½½æ•°æ®
df = pd.read_csv('cell_features.csv')
df.columns = df.columns.str.strip()  # æ¸…ç†åˆ—å
df['frame'] = pd.to_numeric(df['frame'], errors='coerce')
df = df.dropna(subset=['frame'])  # åˆ é™¤ NaN

# 2ï¸âƒ£ é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
features = ['x', 'y', 'area', 'length', 'mean_intensity']
target = 'frame'

# 3ï¸âƒ£ å½’ä¸€åŒ–æ•°æ®
scaler_X = MinMaxScaler()
df[features] = scaler_X.fit_transform(df[features])

scaler_y = MinMaxScaler()
df[['frame']] = scaler_y.fit_transform(df[['frame']])

# 4ï¸âƒ£ å¤„ç† LSTM è¾“å…¥æ ¼å¼
X, y = [], []
window_size = 50  # çª—å£å¤§å°

for i in range(window_size, len(df)):
    X.append(df[features].iloc[i - window_size:i].values)
    y.append(df[target].iloc[i])

X = np.array(X)
y = np.array(y)

# 5ï¸âƒ£ æ•°æ®é›†åˆ’åˆ†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# 6ï¸âƒ£ æ­å»º LSTM ç½‘ç»œ
model = Sequential([
    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)  # é¢„æµ‹å•ä¸ªå€¼
])

# 7ï¸âƒ£ ç¼–è¯‘æ¨¡å‹
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')
model.summary()

# 8ï¸âƒ£ è®­ç»ƒæ¨¡å‹
history = model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=(X_test, y_test))

# 9ï¸âƒ£ ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# ğŸ”Ÿ é¢„æµ‹ & åå½’ä¸€åŒ–
y_pred = model.predict(X_test)

# **ç¡®ä¿ y_pred å’Œ y_test éƒ½è¢«åå½’ä¸€åŒ–**
y_pred = scaler_y.inverse_transform(y_pred)  # é¢„æµ‹å€¼åå½’ä¸€åŒ–
y_test = scaler_y.inverse_transform(y_test.reshape(-1, 1))  # çœŸå®å€¼åå½’ä¸€åŒ–

# è®¡ç®—å‡æ–¹è¯¯å·®
mse = np.mean((y_pred - y_test) ** 2)
print(f'Mean Squared Error: {mse}')

# 1ï¸âƒ£1ï¸âƒ£ ç»˜åˆ¶é¢„æµ‹ vs çœŸå®å€¼
plt.figure(figsize=(10, 6))
plt.plot(y_test, label='True Values')
plt.plot(y_pred, label='Predicted Values')
plt.title('True vs Predicted Cell Division Time')
plt.xlabel('Sample Index')
plt.ylabel('Division Time (Frame)')
plt.legend()
plt.show()
